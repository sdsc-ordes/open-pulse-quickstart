{
  "link": "https://github.com/NVIDIA-NeMo",
  "type": "organization",
  "parsedTimestamp": "2025-11-07T09:11:30.301254",
  "output": {
    "name": null,
    "organizationType": "Industry Company",
    "githubOrganizationMetadata": {
      "login": "NVIDIA-NeMo",
      "name": null,
      "description": "",
      "email": null,
      "location": null,
      "company": null,
      "blog": "https://nvidia.com/",
      "twitter_username": null,
      "public_repos": 14,
      "public_gists": 0,
      "followers": 210,
      "following": 0,
      "created_at": "2025-05-27T19:36:58Z",
      "updated_at": "2025-09-10T14:02:32Z",
      "avatar_url": "https://avatars.githubusercontent.com/u/213689629?v=4",
      "html_url": "https://github.com/NVIDIA-NeMo",
      "gravatar_id": null,
      "type": "Organization",
      "node_id": "O_kgDODLylHQ",
      "url": "https://api.github.com/orgs/NVIDIA-NeMo",
      "repos_url": "https://api.github.com/orgs/NVIDIA-NeMo/repos",
      "events_url": "https://api.github.com/orgs/NVIDIA-NeMo/events",
      "hooks_url": "https://api.github.com/orgs/NVIDIA-NeMo/hooks",
      "issues_url": "https://api.github.com/orgs/NVIDIA-NeMo/issues",
      "members_url": "https://api.github.com/orgs/NVIDIA-NeMo/members{/member}",
      "public_members": [
        "ahmadki",
        "fanshiqing",
        "joyang-nv",
        "jrbourbeau",
        "shan18",
        "trebedea"
      ],
      "repositories": [
        "RL",
        "Skills",
        "NeMo",
        "Guardrails",
        "Megatron-Bridge",
        "Curator",
        "Automodel",
        "Run",
        "Nemotron",
        "Evaluator",
        "Emerging-Optimizers",
        "Export-Deploy",
        "FW-CI-templates",
        ".github"
      ],
      "teams": [],
      "readme_url": "https://github.com/NVIDIA-NeMo/.github/blob/main/profile/README.md",
      "readme_content": "<!--\nSPDX-FileCopyrightText: Copyright (c) 2024-2025 NVIDIA CORPORATION & AFFILIATES. All rights reserved.\nSPDX-License-Identifier: Apache-2.0\n-->\n\n## NVIDIA NeMo Framework Overview\n\nNeMo Framework is NVIDIA's GPU accelerated, end-to-end training framework for large language models (LLMs), multi-modal models and speech models. It enables seamless scaling of training (both pretraining and post-training) workloads from single GPU to thousand-node clusters for both ü§óHugging Face/PyTorch and Megatron models. This GitHub organization includes a suite of libraries and recipe collections to help users train models from end to end. \n\nNeMo Framework is also a part of the NVIDIA NeMo software suite for managing the AI agent lifecycle.\n\n## Latest üì£ announcements and üó£Ô∏è discussions \n### üê≥ NeMo AutoModel\n- [10/6/2025][Enabling PyTorch Native Pipeline Parallelism for ü§ó Hugging Face Transformer Models](https://github.com/NVIDIA-NeMo/Automodel/discussions/589)\n- [9/22/2025][Fine-tune Hugging Face Models Instantly with Day-0 Support with NVIDIA NeMo AutoModel](https://github.com/NVIDIA-NeMo/Automodel/discussions/477)\n- [9/18/2025][üöÄ NeMo Framework Now Supports Google Gemma 3n: Efficient Multimodal Fine-tuning Made Simple](https://github.com/NVIDIA-NeMo/Automodel/discussions/494)\n\n### üî¨ NeMo RL\n- [10/1/2025][On-policy Distillation](https://github.com/NVIDIA-NeMo/RL/discussions/1445)\n- [9/27/2025][FP8 Quantization in NeMo RL](https://github.com/NVIDIA-NeMo/RL/discussions/1216)\n- [8/15/2025][NeMo-RL: Journey of Optimizing Weight Transfer in Large MoE Models by 10x](https://github.com/NVIDIA-NeMo/RL/discussions/1189)\n- [7/31/2025][NeMo-RL V0.3: Scalable and Performant Post-training with Nemo-RL via Megatron-Core](https://github.com/NVIDIA-NeMo/RL/discussions/1161)\n- [5/15/2025][Reinforcement Learning with NVIDIA NeMo-RL: Reproducing a DeepScaleR Recipe Using GRPO ](https://github.com/NVIDIA-NeMo/RL/discussions/1188)\n\n### üí¨ NeMo Speech\n- [8/1/2025][Guide to Fine-tune Nvidia NeMo models with Granary Data](https://github.com/NVIDIA-NeMo/NeMo/discussions/14758)\n\nMore to come and stay tuned!\n\n## Repo organization under NeMo Framework\n  ![image](/RepoDiagram.png)\n  \n<div align=\"center\">\n  Figure 1. NeMo Framework Repo Overview\n</div>\n<p></p>\n\n## Summary of key functionalities and container strategy of each repo\n\nVisit the individual repos to find out more üîç, raise :bug:, contribute ‚úçÔ∏è and participate in discussion forums üó£Ô∏è!\n<p></p>\n\n|Repo|Summary|Training Loop|Training Backends|Infernece Backends|Model Coverage|Container|\n|-|-|-|-|-|-|-|\n|[NeMo Megatron-Bridge](https://github.com/NVIDIA-NeMo/Megatron-Bridge)|Pretraining, LoRA, SFT|PyT native loop|Megatron-core|NA|LLM & VLM|NeMo Framework Container\n|[NeMo AutoModel](https://github.com/NVIDIA-NeMo/Automodel)|Pretraining, LoRA, SFT|PyT native loop|PyTorch|NA|LLM, VLM, Omni, VFM|NeMo AutoModel Container|\n|[NeMo 1.x & 2.x (with Lightning)->will repurpose to focus on Speech](https://github.com/NVIDIA-NeMo/NeMo)|Pretraining,SFT|PyTorch Lightning Loop|Megatron-core & PyTorch|RIVA|Speech|NA|\n|[NeMo RL](https://github.com/NVIDIA-NeMo/RL)|SFT, RL|PyT native loop|Megatron-core & PyTorch|vLLM|LLM, VLM|NeMo RL container|\n|NeMo Gym (WIP)|RL Environment, integrate with RL Framework|NA|NA|NA|NA|NeMo RL Container|\n|[NeMo Aligner (deprecated)](https://github.com/NVIDIA/NeMo-Aligner)|SFT, RL|PyT Lightning Loop|Megatron-core|TRTLLM|LLM|NA\n|[NeMo Curator](https://github.com/NVIDIA-NeMo/Curator)|Data curation|NA|NA|NA|Agnostic|NeMo Curator Container|\n|[NeMo Evaluator](https://github.com/NVIDIA-NeMo/Evaluator)|Model evaluation|NA|NA||Agnostic|NeMo Framework Container|\n|[NeMo Export-Deploy](https://github.com/NVIDIA-NeMo/Export-Deploy)|Export to Production|NA|NA|vLLM, TRT, TRTLLM, ONNX|Agnostic|NeMo Framework Container|\n|[NeMo Run](https://github.com/NVIDIA-NeMo/Run)|Experiment launcher|NA|NA|NA|Agnostic|NeMo Framework Container|\n|[NeMo Guardrails](https://github.com/NVIDIA-NeMo/Guardrails)|Guardrail model response|NA|NA|NA||NA|\n|[NeMo Skills](https://github.com/NVIDIA-NeMo/Skills)|Reference pipeline for SDG & Eval|NA|NA|NA|Agnostic|NA|\n|[NeMo Emerging Optimizers](https://github.com/NVIDIA-NeMo/Emerging-Optimizers)|Collection of Optimizers|NA|Agnostic|NA|NA|NA|\n|NeMo DFM (WIP)|Diffusion foundation model training|PyT native loop|Megatron-core and PyTorch|PyTorch|VFM, Diffusion|TBD|\n|[NeMotron](https://github.com/NVIDIA-NeMo/Nemotron)|Developer asset hub for nemotron models|NA|NA|NA|Nemotron models|NA|\n|NeMo Data-designer (WIP)|Synthetic data generation toolkit|NA|NA|NA|NA|NA|\n\n\n<div align=\"center\">\n  Table 1. NeMo Framework Repos\n</div>\n<p></p>\n\n### Some background contexts and motivations\nThe NeMo GitHub Org and its repo collections are created to address the following problems\n* **Need for composability**: The [Previous NeMo](https://github.com/NVIDIA/NeMo) is monolithic and encompasses too many things, making it hard for users to find what they need. Container size is also an issue. Breaking down the Monolithic repo into a series of functional-focused repos to facilitate code discovery.\n* **Need for customizability**: The [Previous NeMo](https://github.com/NVIDIA/NeMo) uses PyTorch Lighting as the default trainer loop, which provides some out of the box functionality but making it hard to customize. [NeMo Megatron-Bridge](https://github.com/NVIDIA-NeMo/Megatron-Bridge), [NeMo AutoModel](https://github.com/NVIDIA-NeMo/Automodel), and [NeMo RL](https://github.com/NVIDIA-NeMo/RL) have adopted pytorch native custom loop to improve flexibility and ease of use for developers. \n\n## Documentation\n\nTo learn more about NVIDIA NeMo Framework and all of its component libraries, please refer to the [NeMo Framework User Guide](https://docs.nvidia.com/nemo-framework/user-guide/latest/overview.html), which includes quick start guide, tutorials, model-specific recipes, best practice guides and performance benchmarks.  \n\n<!--\n## Contribution & Support\n\n- Follow [Contribution Guidelines](../CONTRIBUTING.md)\n- Report issues via GitHub Discussions\n- Enterprise support available through NVIDIA AI Enterprise\n-->\n\n## License\n\nApache 2.0 licensed with third-party attributions documented in each repository.\n",
      "social_accounts": [],
      "pinned_repositories": []
    },
    "organizationTypeJustification": "The organization is owned by NVIDIA (evidenced by the NVIDIA copyright header and blog domain nvidia.com), focuses on commercial-grade software frameworks, and operates under a corporate Apache 2.0 license.",
    "description": "NVIDIA NeMo Framework is NVIDIA's GPU-accelerated, end-to-end training framework for large language models, multi-modal models, speech models, and reinforcement learning. It supports seamless scaling from a single GPU to multi-node clusters for Hugging Face/PyTorch and Megatron models. The organization maintains a modular suite of libraries and recipe collections for pretraining, fine-tuning, data curation, evaluation, export/deployment, and experiment orchestration, packaged in containerized environments.",
    "relatedToOrganization": [
      "Nvidia (United States)",
      {
        "type": "Organization",
        "legalName": "Nvidia (United States)",
        "hasRorId": "https://ror.org/03jdj4y14",
        "alternateNames": [
          "NVIDIA Corporation"
        ],
        "organizationType": "Company",
        "parentOrganization": null,
        "country": "United States",
        "website": "http://www.nvidia.com/page/home.html",
        "attributionConfidence": 0.8,
        "academicCatalogRelations": null
      }
    ],
    "relatedToOrganizationJustification": [],
    "discipline": [
      "Computer engineering",
      "Information engineering",
      "Systems science and engineering"
    ],
    "disciplineJustification": [
      "The development of a GPU-accelerated training framework and custom software loops indicates expertise in Computer engineering.",
      "Management of large-scale data pipelines, model I/O, and API design reflects core Information engineering principles.",
      "Designing and orchestrating distributed training across clusters and containers involves Systems science and engineering.",
      "], ",
      ""
    ],
    "relatedToEPFL": false,
    "relatedToEPFLJustification": "Comprehensive Justification:\n\n1) Organization name does not contain ‚ÄúEPFL‚Äù (no name-based EPFL mention).\n2) Description does not mention EPFL, SDSC, or Swiss Data Science Center.\n3) README content for the NVIDIA-NeMo organization contains no reference to EPFL or related initiatives.\n4) Website/blog domain is nvidia.com, not epfl.ch.\n5) No ROR entries or parentOrganization links to EPFL or joint EPFL/ETH entities.\n6) GitHub metadata has no location in Lausanne, Switzerland, nor any EPFL-related location.\n7) No members or related organizations have EPFL email domains or affiliations.\n8) No Infoscience entities found for this organization in EPFL‚Äôs repository.\n\nNo positive evidence was identified, resulting in a cumulative confidence score of 0.0 (<0.5).",
    "relatedToEPFLConfidence": 0.0,
    "academicCatalogRelations": []
  },
  "stats": {
    "agent_input_tokens": 0,
    "agent_output_tokens": 0,
    "total_tokens": 0,
    "estimated_input_tokens": 8944,
    "estimated_output_tokens": 493,
    "estimated_total_tokens": 9437,
    "duration": 83.685715,
    "start_time": "2025-11-07T09:10:06.615220",
    "end_time": "2025-11-07T09:11:30.300935",
    "status_code": 200,
    "github_rate_limit": 5000,
    "github_rate_remaining": 4814,
    "github_rate_reset": "2025-11-07T09:40:18"
  }
}